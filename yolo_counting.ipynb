{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object Detecion \n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "#plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#basics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Display image and videos\n",
    "import IPython\n",
    "from IPython.display import Video, display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading a YOLO model \n",
    "model = YOLO('/home/raza.imam/Documents/FRC/YOLOv8/runs/detect/train/weights/best.pt')\n",
    "\n",
    "#geting names from classes\n",
    "dict_classes = model.model.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'camel', 1: 'mask', 2: 'pole', 3: 'rope'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_IDS = [0]\n",
    "frame_path = '/home/raza.imam/Documents/FRC/YOLOv8/datasets/Camel-object-detection-MAIN-2/test/images/Camel_6_jpg.rf.3642acb12e8961266c3b0318e6b275b4.jpg'\n",
    "frame = cv2.imread(frame_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) /io/opencv/modules/highgui/src/window.cpp:1272: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m/home/raza.imam/Documents/FRC/yolo_counting.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 48>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/raza.imam/Documents/FRC/yolo_counting.ipynb#W3sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m frames_list\u001b[39m.\u001b[39mappend(frame)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/raza.imam/Documents/FRC/yolo_counting.ipynb#W3sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39m#printing the frame\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/raza.imam/Documents/FRC/yolo_counting.ipynb#W3sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m cv2\u001b[39m.\u001b[39;49mimshow(\u001b[39m'\u001b[39;49m\u001b[39mframe\u001b[39;49m\u001b[39m'\u001b[39;49m, frame)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/raza.imam/Documents/FRC/yolo_counting.ipynb#W3sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m cv2\u001b[39m.\u001b[39mwaitKey(\u001b[39m0\u001b[39m)\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.7.0) /io/opencv/modules/highgui/src/window.cpp:1272: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n"
     ]
    }
   ],
   "source": [
    "veiculos_contador_in = dict.fromkeys(class_IDS, 0)\n",
    "veiculos_contador_out = dict.fromkeys(class_IDS, 0)\n",
    "frames_list = []\n",
    "\n",
    "y_hat = model.predict(frame, conf = 0.25, classes = class_IDS, device = 0, verbose = False)\n",
    "    \n",
    "    # Getting the bounding boxes, confidence and classes of the recognize objects in the current frame.\n",
    "boxes   = y_hat[0].boxes.xyxy.cpu().numpy()\n",
    "conf    = y_hat[0].boxes.conf.cpu().numpy()\n",
    "classes = y_hat[0].boxes.cls.cpu().numpy()\n",
    "\n",
    "positions_frame = pd.DataFrame(y_hat[0].cpu().numpy().boxes.boxes, columns = ['xmin', 'ymin', 'xmax', 'ymax', 'conf', 'class'])\n",
    "    \n",
    "#Translating the numeric class labels to text\n",
    "labels = [dict_classes[i] for i in classes]\n",
    "\n",
    "for ix, row in enumerate(positions_frame.iterrows()):\n",
    "        # Getting the coordinates of each vehicle (row)\n",
    "        xmin, ymin, xmax, ymax, confidence, category,  = row[1].astype('int')\n",
    "        \n",
    "        # Calculating the center of the bounding-box\n",
    "        center_x, center_y = int(((xmax+xmin))/2), int((ymax+ ymin)/2)\n",
    "        \n",
    "        # drawing center and bounding-box of vehicle in the given frame \n",
    "        cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (255,0,0), 5) # box\n",
    "        cv2.circle(frame, (center_x,center_y), 5,(255,0,0),-1) # center of box\n",
    "        \n",
    "        #Drawing above the bounding-box the name of class recognized.\n",
    "        cv2.putText(img=frame, text=labels[ix]+' - '+str(np.round(conf[ix],2)),\n",
    "                    org= (xmin,ymin-10), fontFace=cv2.FONT_HERSHEY_TRIPLEX, fontScale=1, color=(255, 0, 0),thickness=2)\n",
    "        \n",
    "        \n",
    "#updating the counting type of vehicle \n",
    "contador_in_plt = [f'{dict_classes[k]}: {i}' for k, i in veiculos_contador_in.items()]\n",
    "contador_out_plt = [f'{dict_classes[k]}: {i}' for k, i in veiculos_contador_out.items()]\n",
    "\n",
    "#drawing the number of vehicles in\\out\n",
    "cv2.putText(img=frame, text='N. vehicles In', \n",
    "            org= (30,30), fontFace=cv2.FONT_HERSHEY_TRIPLEX, \n",
    "            fontScale=1, color=(255, 255, 0),thickness=1)\n",
    "\n",
    "cv2.putText(img=frame, text='N. vehicles Out', \n",
    "            org= (int(2800 * 5/100 ),30), \n",
    "            fontFace=cv2.FONT_HERSHEY_TRIPLEX, fontScale=1, color=(255, 255, 0),thickness=1)\n",
    "frames_list.append(frame)\n",
    "        \n",
    "#printing the frame\n",
    "cv2.imshow('frame', frame)\n",
    "cv2.waitKey(0)\n",
    "#cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary functions\n",
    "def risize_frame(frame, scale_percent):\n",
    "    \"\"\"Function to resize an image in a percent scale\"\"\"\n",
    "    width = int(frame.shape[1] * scale_percent / 100)\n",
    "    height = int(frame.shape[0] * scale_percent / 100)\n",
    "    dim = (width, height)\n",
    "\n",
    "    # resize image\n",
    "    resized = cv2.resize(frame, dim, interpolation = cv2.INTER_AREA)\n",
    "    return resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] - Verbose during Prediction: False\n",
      "[INFO] - Original Dim:  (3840, 2160)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x5634504d/'MP4V' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3536ad9a7fcd48e9ba7c7fe7901fbda0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5348 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[hevc @ 0x7f8e3fb239c0] Could not find ref with POC 0\n",
      "[hevc @ 0x7f8d6a817f00] Could not find ref with POC 37\n",
      "[hevc @ 0x7f8eecdcd740] Could not find ref with POC 0\n",
      "[hevc @ 0x7f8e45c0e500] Could not find ref with POC 31\n",
      "[hevc @ 0x7e17580] Could not find ref with POC 13\n",
      "[hevc @ 0x7f8d8c9586c0] Could not find ref with POC 18\n",
      "[hevc @ 0x803d2c0] Could not find ref with POC 32\n",
      "[hevc @ 0x803d2c0] Could not find ref with POC 11\n",
      "[hevc @ 0x7e17580] Could not find ref with POC 38\n",
      "[hevc @ 0x7d7f940] Could not find ref with POC 19\n",
      "[hevc @ 0x7e17580] Could not find ref with POC 29\n"
     ]
    }
   ],
   "source": [
    "### Configurations\n",
    "#Verbose during prediction\n",
    "verbose = False\n",
    "# Scaling percentage of original frame\n",
    "scale_percent = 100\n",
    "\n",
    "\n",
    "#-------------------------------------------------------\n",
    "# Reading video with cv2\n",
    "video = cv2.VideoCapture('/home/muhammad.huzaifa/Documents/Research/Internship/data/H.mp4')\n",
    "\n",
    "# Objects to detect Yolo\n",
    "class_IDS = [0] \n",
    "# Auxiliary variables\n",
    "centers_old = {}\n",
    "centers_new = {}\n",
    "obj_id = 0 \n",
    "veiculos_contador_in = dict.fromkeys(class_IDS, 0)\n",
    "veiculos_contador_out = dict.fromkeys(class_IDS, 0)\n",
    "end = []\n",
    "frames_list = []\n",
    "cy_linha = int(1500 * scale_percent/100 )\n",
    "cx_sentido = int(2000 * scale_percent/100) \n",
    "offset = int(8 * scale_percent/100 )\n",
    "contador_in = 0\n",
    "contador_out = 0\n",
    "print(f'[INFO] - Verbose during Prediction: {verbose}')\n",
    "\n",
    "\n",
    "# Original informations of video\n",
    "height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "fps = video.get(cv2.CAP_PROP_FPS)\n",
    "print('[INFO] - Original Dim: ', (width, height))\n",
    "\n",
    "# Scaling Video for better performance \n",
    "if scale_percent != 100:\n",
    "    print('[INFO] - Scaling change may cause errors in pixels lines ')\n",
    "    width = int(width * scale_percent / 100)\n",
    "    height = int(height * scale_percent / 100)\n",
    "    print('[INFO] - Dim Scaled: ', (width, height))\n",
    "    \n",
    "\n",
    "#-------------------------------------------------------\n",
    "### Video output ####\n",
    "video_name = 'result.mp4'\n",
    "output_path = \"rep_\" + video_name\n",
    "tmp_output_path = \"tmp_\" + output_path\n",
    "VIDEO_CODEC = \"MP4V\"\n",
    "\n",
    "output_video = cv2.VideoWriter(tmp_output_path, \n",
    "                               cv2.VideoWriter_fourcc(*VIDEO_CODEC), \n",
    "                               fps, (width, height))\n",
    "\n",
    "\n",
    "#-------------------------------------------------------\n",
    "# Executing Recognition \n",
    "for i in tqdm(range(int(video.get(cv2.CAP_PROP_FRAME_COUNT)))):\n",
    "    \n",
    "    # reading frame from video\n",
    "    _, frame = video.read()\n",
    "    \n",
    "    #Applying resizing of read frame\n",
    "    frame  = risize_frame(frame, scale_percent)\n",
    "    \n",
    "    if verbose:\n",
    "        print('Dimension Scaled(frame): ', (frame.shape[1], frame.shape[0]))\n",
    "\n",
    "    # Getting predictions\n",
    "    y_hat = model.predict(frame, conf = 0.25, classes = class_IDS, device = 0, verbose = False)\n",
    "    \n",
    "    # Getting the bounding boxes, confidence and classes of the recognize objects in the current frame.\n",
    "    boxes   = y_hat[0].boxes.xyxy.cpu().numpy()\n",
    "    conf    = y_hat[0].boxes.conf.cpu().numpy()\n",
    "    classes = y_hat[0].boxes.cls.cpu().numpy() \n",
    "    \n",
    "    # Storing the above information in a dataframe\n",
    "    positions_frame = pd.DataFrame(y_hat[0].cpu().numpy().boxes.boxes, columns = ['xmin', 'ymin', 'xmax', 'ymax', 'conf', 'class'])\n",
    "    \n",
    "    #Translating the numeric class labels to text\n",
    "    labels = [dict_classes[i] for i in classes]\n",
    "    \n",
    "    # Drawing transition line for in\\out vehicles counting \n",
    "    cv2.line(frame, (0, cy_linha), (int(4500 * scale_percent/100 ), cy_linha), (255,255,0),8)\n",
    "    \n",
    "    # For each vehicles, draw the bounding-box and counting each one the pass thought the transition line (in\\out)\n",
    "    for ix, row in enumerate(positions_frame.iterrows()):\n",
    "        # Getting the coordinates of each vehicle (row)\n",
    "        xmin, ymin, xmax, ymax, confidence, category,  = row[1].astype('int')\n",
    "        \n",
    "        # Calculating the center of the bounding-box\n",
    "        center_x, center_y = int(((xmax+xmin))/2), int((ymax+ ymin)/2)\n",
    "        \n",
    "        # drawing center and bounding-box of vehicle in the given frame \n",
    "        cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (255,0,0), 5) # box\n",
    "        cv2.circle(frame, (center_x,center_y), 5,(255,0,0),-1) # center of box\n",
    "        \n",
    "        #Drawing above the bounding-box the name of class recognized.\n",
    "        cv2.putText(img=frame, text=labels[ix]+' - '+str(np.round(conf[ix],2)),\n",
    "                    org= (xmin,ymin-10), fontFace=cv2.FONT_HERSHEY_TRIPLEX, fontScale=1, color=(255, 0, 0),thickness=2)\n",
    "        \n",
    "        # Checking if the center of recognized vehicle is in the area given by the transition line + offset and transition line - offset \n",
    "        if (center_y < (cy_linha + offset)) and (center_y > (cy_linha - offset)):\n",
    "            if  (center_x >= 0) and (center_x <=cx_sentido):\n",
    "                contador_in +=1\n",
    "                veiculos_contador_in[category] += 1\n",
    "            else:\n",
    "                contador_out += 1\n",
    "                veiculos_contador_out[category] += 1\n",
    "    \n",
    "    #updating the counting type of vehicle \n",
    "    contador_in_plt = [f'{dict_classes[k]}: {i}' for k, i in veiculos_contador_in.items()]\n",
    "    contador_out_plt = [f'{dict_classes[k]}: {i}' for k, i in veiculos_contador_out.items()]\n",
    "    \n",
    "    #drawing the number of vehicles in\\out\n",
    "    cv2.putText(img=frame, text='N. vehicles In', \n",
    "                org= (30,30), fontFace=cv2.FONT_HERSHEY_TRIPLEX, \n",
    "                fontScale=1, color=(255, 255, 0),thickness=1)\n",
    "    \n",
    "    cv2.putText(img=frame, text='N. vehicles Out', \n",
    "                org= (int(2800 * scale_percent/100 ),30), \n",
    "                fontFace=cv2.FONT_HERSHEY_TRIPLEX, fontScale=1, color=(255, 255, 0),thickness=1)\n",
    "    \n",
    "    #drawing the counting of type of vehicles in the corners of frame \n",
    "    xt = 40\n",
    "    for txt in range(len(contador_in_plt)):\n",
    "        xt +=30\n",
    "        cv2.putText(img=frame, text=contador_in_plt[txt], \n",
    "                    org= (30,xt), fontFace=cv2.FONT_HERSHEY_TRIPLEX, \n",
    "                    fontScale=1, color=(255, 255, 0),thickness=1)\n",
    "        \n",
    "        cv2.putText(img=frame, text=contador_out_plt[txt],\n",
    "                    org= (int(2800 * scale_percent/100 ),xt), fontFace=cv2.FONT_HERSHEY_TRIPLEX,\n",
    "                    fontScale=1, color=(255, 255, 0),thickness=1)\n",
    "    \n",
    "    #drawing the number of vehicles in\\out\n",
    "    cv2.putText(img=frame, text=f'In:{contador_in}', \n",
    "                org= (int(1820 * scale_percent/100 ),cy_linha+60),\n",
    "                fontFace=cv2.FONT_HERSHEY_TRIPLEX, fontScale=1, color=(255, 255, 0),thickness=2)\n",
    "    \n",
    "    cv2.putText(img=frame, text=f'Out:{contador_out}', \n",
    "                org= (int(1800 * scale_percent/100 ),cy_linha-40),\n",
    "                fontFace=cv2.FONT_HERSHEY_TRIPLEX, fontScale=1, color=(255, 255, 0),thickness=2)\n",
    "\n",
    "    if verbose:\n",
    "        print(contador_in, contador_out)\n",
    "    #Saving frames in a list \n",
    "    frames_list.append(frame)\n",
    "    #saving transformed frames in a output video formaat\n",
    "    output_video.write(frame)\n",
    "    \n",
    "#Releasing the video    \n",
    "output_video.release()\n",
    "\n",
    "\n",
    "####  pos processing\n",
    "# Fixing video output codec to run in the notebook\\browser\n",
    "if os.path.exists(output_path):\n",
    "    os.remove(output_path)\n",
    "    \n",
    "subprocess.run(\n",
    "    [\"ffmpeg\",  \"-i\", tmp_output_path,\"-crf\",\"18\",\"-preset\",\"veryfast\",\"-hide_banner\",\"-loglevel\",\"error\",\"-vcodec\",\"libx264\",output_path])\n",
    "os.remove(tmp_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking samples of processed frames\n",
    "for i in [28, 29, 32, 40, 42, 50, 58]:\n",
    "    plt.figure(figsize =( 14, 10))\n",
    "    plt.imshow(frames_list[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object Detecion \n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "#plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#basics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Display image and videos\n",
    "import IPython\n",
    "from IPython.display import Video, display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.20 🚀 Python-3.10.9 torch-1.13.1+cu117 CUDA:0 (Quadro RTX 6000, 24190MiB)\n",
      "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Loading a YOLO model\n",
    "model = YOLO('/home/muhammad.huzaifa/Documents/Research/Internship/runs/detect/train3/weights/best.pt')\n",
    "\n",
    "# Getting names from classes\n",
    "dict_classes = model.model.names\n",
    "\n",
    "class_IDS = [0]\n",
    "frame_path = 'data_img/frame1_2760.jpg'\n",
    "frame = cv2.imread(frame_path)\n",
    "\n",
    "y_hat = model.predict(frame, conf=0.45, classes=class_IDS, device=0, verbose=False)\n",
    "\n",
    "# Getting the bounding boxes, confidence, and classes of the recognized objects in the current frame.\n",
    "boxes = y_hat[0].boxes.xyxy.cpu().numpy()\n",
    "conf = y_hat[0].boxes.conf.cpu().numpy()\n",
    "classes = y_hat[0].boxes.cls.cpu().numpy()\n",
    "\n",
    "# Iterate over the bounding boxes and annotate them on the frame\n",
    "for box, confidence, class_id in zip(boxes, conf, classes):\n",
    "    xmin, ymin, xmax, ymax = box.astype(int)\n",
    "    label = dict_classes[class_id]\n",
    "    \n",
    "    # Draw the bounding box\n",
    "    cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "    \n",
    "    # Add label and confidence\n",
    "    text = f\"{label} ({confidence:.2f})\"\n",
    "    cv2.putText(frame, text, (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "# Display the final annotated image\n",
    "#cv2.imshow(\"Annotated Image\", frame)\n",
    "\n",
    "# Count the number of bounding boxes\n",
    "box_count = len(boxes)\n",
    "\n",
    "# Display the count on the side of the image\n",
    "#cv2.putText(frame, f\"Number of Workers: {box_count}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 2.0, (0, 0, 255), 2)\n",
    "text = f\"Number of Workers: {box_count}\"\n",
    "text_size, _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 3.2, 2)\n",
    "\n",
    "text_x = 10\n",
    "text_y = frame.shape[0] - 10\n",
    "\n",
    "cv2.putText(frame, text, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 3.2, (0, 0, 255), 3)\n",
    "\n",
    "\n",
    "# saving the image\n",
    "cv2.imwrite('ttt.jpg', frame)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:04:18) \n[GCC 10.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a817753b218724dcb375305fbfddbb2a636cdfec48b741162f9042124e885d09"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
